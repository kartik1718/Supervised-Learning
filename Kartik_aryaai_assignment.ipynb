{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pandas_profiling import ProfileReport\n",
    "import datetime as DT\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import statistics\n",
    "from time import clock\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reading and some basic info about the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read=pd.read_csv(\"training_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read.head()\n",
    "data_read = data_read.drop(data_read.columns[0],axis='columns') #dropping the unneccesary index at position 0\n",
    "data_read.info() #Feature X1 to X55 contains float values while X56 and X57 have int values, and X58 it the output feature or dependent variable\n",
    "data_read.describe() #There are some max values which serves as outliers in the X55,X56,X57 feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(data_read, title=\"Pandas Profiling Report\")\n",
    "profile\n",
    "# Then we get to know that there are no missing values in the dataset \n",
    "# Whereas most of the columns have average 80-85% of zeroes in that\n",
    "# While X34 is highly correlated with the X32 so we should drop any of it.\n",
    "# 2376 values are identified as class'0' whereas 1534 values are from class '1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have already seen the distribution of the data in profiling we will continue with the correlation matrix\n",
    "def plotCorrelationMatrix(df, graphWidth):\n",
    "    #filename = df.dataframeName\n",
    "    df = df.dropna('columns') # drop columns with NaN\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    if df.shape[1] < 2:\n",
    "        print('No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
    "        return\n",
    "    corr = df.corr()\n",
    "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
    "    corrMat = plt.matshow(corr, fignum = 1)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.colorbar(corrMat)\n",
    "    plt.title('Correlation Matrix', fontsize=15)\n",
    "    plt.show()\n",
    "    \n",
    "## Scatter and density plots\n",
    "def plotScatterMatrix(df, plotSize, textSize):\n",
    "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
    "    # Remove rows and columns that would lead to df being singular\n",
    "    df = df.dropna('columns')\n",
    "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
    "    columnNames = list(df)\n",
    "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
    "        columnNames = columnNames[:10]\n",
    "    df = df[columnNames]\n",
    "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
    "    corrs = df.corr().values\n",
    "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
    "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
    "    plt.suptitle('Scatter and Density Plot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCorrelationMatrix(data_read, 15) # There is tight corelation between 32 and 34 but slight less corelation between feature 28 to 38\n",
    "plotScatterMatrix(data_read, 20, 15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature preprocessing and selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we know there is only two features that are strongly corelated, we will remove any of them\n",
    "data_read=data_read.drop('X32',axis='columns') \n",
    "\n",
    "#Getting the shape of dataframe\n",
    "print(\"shape of the data:\", data_read.shape)\n",
    "\n",
    "#We don't need to encode into numerical as all the columns are numerical \n",
    "\n",
    "#Splitting the data with 4:1(80:20)\n",
    "X = data_read.iloc[:,0:56]\n",
    "y = data_read['Y']\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "#for lda\n",
    "#X_train, X_eval, y_train, y_eval = train_test_split(features_new, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above LDA is just for analysis. No need to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For splitting with the LDA in case for checking variance in the dataset\n",
    "LDA_transform = LDA(n_components=1)\n",
    "LDA_transform.fit(X, y)\n",
    "features_new = LDA_transform.transform(X)\n",
    "\n",
    "# Print the number of features\n",
    "print('Original feature #:', X.shape[1])\n",
    "print('Reduced feature #:', features_new.shape[1])\n",
    "\n",
    "# Print the ratio of explained variance\n",
    "print(LDA_transform.explained_variance_ratio_)\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(features_new, y, test_size=0.20, random_state=42)\n",
    "\n",
    "#And then train it with the Logistic Regression,DT and Random Forest models\n",
    "#But Logistic Regression slightly performed better after new feature set.\n",
    "#But the Tree classifiers have done poorly on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will see some feature selection methods for automatically reducing the features according to the variance\n",
    "sel_variance_threshold = VarianceThreshold() \n",
    "X_train_remove_variance = sel_variance_threshold.fit_transform(X_train)\n",
    "print(X_train_remove_variance.shape)\n",
    "del X_train_remove_variance\n",
    "#Shape of X_train and reduced variance train set is same so no feature is reduced. Thus we will be using X-train as is.\n",
    "#VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesnâ€™t meet some threshold. By default, it removes all zero-variance features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creating instances of classifiers\n",
    "clf_NB = GaussianNB()\n",
    "clf_DT = DecisionTreeClassifier()\n",
    "clf_LR = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default Printing statements\n",
    "def printing_statements_upper(tablename,model_name):    \n",
    "    print(\"============\")\n",
    "    print(\"Table %s\" % tablename)\n",
    "    print(\"\\nScores for the %s\" % model_name)\n",
    "    print(\"============\")\n",
    "    print(\"____________________________________________________________________\")\n",
    "    print(\"\\t\\t%s\\t\" % model_name)\n",
    "    print(\"____________________________________________________________________\")\n",
    "\n",
    "def printing_statements_lower(accuracy,model_name,tim,F1):\n",
    "    print(\"Accuracy is : %s\" % accuracy)\n",
    "    print(\"\\nThe training time for %s in seconds \" % model_name)\n",
    "    print(\"============\")\n",
    "    print(\"Model training time is : %s\" % tim)\n",
    "    print(\"\\nThe F-measure/F1 score is \")\n",
    "    print(\"============\")\n",
    "    print(\"F1 score is : %s \" % F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "def model_logistic():\n",
    "    roc_auc_lr=[]\n",
    "    printing_statements_upper('3','Logistic Regression')\n",
    "    start_time = clock()\n",
    "    clf_LR.fit(X_train, y_train)\n",
    "    t_LR=round(clock() - start_time,3)\n",
    "    pred = clf_LR.predict(X_eval)\n",
    "    ##ROC characteristics curve\n",
    "    fpr, tpr, _ = roc_curve(y_eval, clf_LR.predict_proba(X_eval)[:, 1])\n",
    "    roc_auc_lr.append(auc(fpr, tpr))\n",
    "    ##\n",
    "    acc_LR = round(accuracy_score(y_eval, pred),4)\n",
    "    fm_LR = f1_score(y_eval, pred)\n",
    "    printing_statements_lower(acc_LR,'Logistic Regression',t_LR,fm_LR)  \n",
    "\n",
    "model_logistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "def model_training_naive():\n",
    "    roc_auc_nb=[]\n",
    "    printing_statements_upper('1','Naive Bayes')\n",
    "    start_time_naive = clock()\n",
    "    clf_NB.fit(X_train, y_train)\n",
    "    t_NB=round(clock()-start_time_naive,3)\n",
    "    pred_naive = clf_NB.predict(X_eval)\n",
    "    ##ROC characteristics curve\n",
    "    fpr, tpr, _ = roc_curve(y_eval, clf_LR.predict_proba(X_eval)[:, 1])\n",
    "    roc_auc_nb.append(auc(fpr, tpr))\n",
    "    ##\n",
    "    acc_NB = round(accuracy_score(y_eval, pred_naive),4)\n",
    "    fm_NB = f1_score(y_eval, pred_naive)\n",
    "    printing_statements_lower(acc_NB,'Naive Bayes',t_NB,fm_NB)   \n",
    "    \n",
    "    \n",
    "model_training_naive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "def model_training_dt():\n",
    "    roc_auc_dt=[]\n",
    "    printing_statements_upper('2','Decision Tree')\n",
    "    start_time = clock()\n",
    "    clf_DT.fit(X_train, y_train)\n",
    "    t_DT=round(clock() - start_time,3)\n",
    "    pred = clf_DT.predict(X_eval)\n",
    "    ##ROC characterisitcs\n",
    "    fpr, tpr, _ = roc_curve(y_eval, clf_DT.predict_proba(X_eval)[:, 1])\n",
    "    roc_auc_dt.append(auc(fpr, tpr))\n",
    "    ##\n",
    "    acc_DT = round(accuracy_score(y_eval, pred),4)\n",
    "    fm_DT = f1_score(y_eval, pred)\n",
    "    printing_statements_lower(acc_DT,'Decision Tree',t_DT,fm_DT)\n",
    "    \n",
    "model_training_dt()\n",
    "\n",
    "\n",
    "importances = clf_DT.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "shaped_df=X.values\n",
    "for f in range(shaped_df.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    \n",
    "#X51 and X6 have highest feature importance in the prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "def random_for():\n",
    "    roc_auc_rf = []\n",
    "    printing_statements_upper('4','Random Forest')\n",
    "    rf = RandomForestClassifier(n_estimators=73)\n",
    "    #Tried various combinations for the estimators\n",
    "    start_time_rdm = clock()\n",
    "    rf.fit(X_train, y_train)\n",
    "    t_RF=round(clock() - start_time_rdm,3)\n",
    "    y_pred = rf.predict(X_eval)\n",
    "    ####\n",
    "    fpr, tpr, _ = roc_curve(y_eval, rf.predict_proba(X_eval)[:, 1])\n",
    "    roc_auc_rf.append(auc(fpr, tpr))\n",
    "    ####\n",
    "    rf_acc=rf.score(X_eval, y_eval)\n",
    "    print(\"The accuracy is : \",rf_acc)\n",
    "    print(\"Training Time is : \",t_RF)\n",
    "    # Evaluate the confusion_matrix\n",
    "    confusion_matrix(y_eval, y_pred)\n",
    "    print(classification_report(y_eval,y_pred))\n",
    "\n",
    "random_for()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_auc(y_true, y_score,model_name):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic for %s' % model_name)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "#ROC curves for different models    \n",
    "#show_auc(y_eval,rf.predict_proba(X_eval)[:, 1],'Random Forest')\n",
    "show_auc(y_eval,clf_LR.predict_proba(X_eval)[:,1],'Logistic Regression')\n",
    "show_auc(y_eval,clf_DT.predict_proba(X_eval)[:,1],'Decision Tree') ## For decision tree\n",
    "show_auc(y_eval,clf_NB.predict_proba(X_eval)[:,1],'Naive Bayes')\n",
    "\n",
    "#Hence I will go with the Random Forest for testing it with hold out set we have reserved.\n",
    "#So will just save the model file of Random Forest for testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model Only one time activity\n",
    "filename = 'Best_classifier.pckl' \n",
    "pickle.dump(rf, open(filename, 'wb'))\n",
    "\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the sake of speed the naive bayes outperforms all the other models. But the accuracy,F1 and ROC characteristics are better for Random Forest and DT.\n",
    "### So I will choose the Random forest algorithm for the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again want to test some scaling if it works with the data.\n",
    "# Scaling X_train and X_test\n",
    "#So comparing with average performance model to check for accuracy and F1 score\n",
    "#So there is no change in scores while scaling the dataset and training it.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX_train = scaler.fit_transform(X_train)\n",
    "rescaledX_eval = scaler.transform(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_naive():\n",
    "    roc_auc_nb=[]\n",
    "    printing_statements_upper('1','Naive Bayes')\n",
    "    start_time_naive = clock()\n",
    "    clf_NB.fit(rescaledX_train, y_train)\n",
    "    t_NB=round(clock()-start_time_naive,3)\n",
    "    pred_naive = clf_NB.predict(rescaledX_eval)\n",
    "    ##ROC characteristics curve\n",
    "    fpr, tpr, _ = roc_curve(y_eval, clf_LR.predict_proba(rescaledX_eval)[:, 1])\n",
    "    roc_auc_nb.append(auc(fpr, tpr))\n",
    "    ##\n",
    "    acc_NB = round(accuracy_score(y_eval, pred_naive),4)\n",
    "    fm_NB = f1_score(y_eval, pred_naive)\n",
    "    printing_statements_lower(acc_NB,'Naive Bayes',t_NB,fm_NB) \n",
    "    \n",
    "model_training_naive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_dt():\n",
    "    roc_auc_dt=[]\n",
    "    printing_statements_upper('2','Decision Tree')\n",
    "    start_time = clock()\n",
    "    clf_DT.fit(rescaledX_train, y_train)\n",
    "    t_DT=round(clock() - start_time,3)\n",
    "    pred = clf_DT.predict(rescaledX_eval)\n",
    "    ##ROC characterisitcs\n",
    "    fpr, tpr, _ = roc_curve(y_eval, clf_DT.predict_proba(rescaledX_eval)[:, 1])\n",
    "    roc_auc_dt.append(auc(fpr, tpr))\n",
    "    ##\n",
    "    acc_DT = round(accuracy_score(y_eval, pred),4)\n",
    "    fm_DT = f1_score(y_eval, pred)\n",
    "    printing_statements_lower(acc_DT,'Decision Tree',t_DT,fm_DT)\n",
    "    \n",
    "model_training_dt()\n",
    "\n",
    "\n",
    "importances = clf_DT.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "shaped_df=X.values\n",
    "for f in range(shaped_df.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    \n",
    "#X51 and X6 have highest feature importance in the prediction "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
